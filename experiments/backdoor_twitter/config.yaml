# Basic configuration file for running classif_cnn example using hdf5 files.
# Parameters needed to initialize the model
model_config:
    model_type: LSTM                                    # class w/ `loss` and `inference` methods
    model_folder: experiments/backdoor_twitter/model.py     # file containing class
    num_classes: 2

# Configuration for differential privacy
dp_config:
    enable_local_dp: false                             # whether to enable user-level DP

# Additional privacy metrics
privacy_metrics_config:
    apply_metrics: false                               # cache data to compute additional metrics

# Select the Federated optimizer to use (e.g. DGA or FedAvg)
strategy: FedAvg

# Determines all the server-side settings for training and evaluation rounds
server_config:   
    wantRL: false                                      # whether to use RL-based meta-optimizers
    resume_from_checkpoint: false                      # restart from checkpoint if file exists
    do_profiling: false                                # run profiler and compute runtime metrics
    optimizer_config:                                  # this is the optimizer used to update the model
        type: sgd
        lr: 1.0
        weight_decay: 0.00000
    annealing_config:                                  # annealer for the learning rate
        type: step_lr
        step_interval: epoch
        gamma: 1.0
        step_size: 100
    val_freq: 1                                         # how many iterations between metric eval on val set
    rec_freq: 1                                         # how many iterations between metric eval on test set
    initial_val: true
    initial_rec: true
    max_iteration: 500                                 # how many iterations in total
    num_clients_per_iteration: 20                      # how many clients per iteration
    data_config:                                       # where to get val and test data from
        val:
            batch_size: 10000
            loader_type: text
            val_data: test_data.hdf5
        test:
            batch_size: 10000
            loader_type: text
            test_data: test_data.hdf5
    type: model_optimization
    aggregate_median: softmax                          # how aggregations weights are computed
    initial_lr_client: 0.0001                            # learning rate used on client optimizer
    lr_decay_factor: 1.0
    weight_train_loss: train_loss
    best_model_criterion: f1_score
    fall_back_to_best_model: false
    softmax_beta: 1.0

# Dictates the learning parameters for client-side model updates. Train data is defined inside this config.
client_config:
    do_profiling: false                                # run profiling and compute runtime metrics
    ignore_subtask: false
    data_config:                                       # where to get training data from
        train:
            batch_size: 64
            loader_type: text
            list_of_train_data: train_data.hdf5
            desired_max_samples: 50000
    optimizer_config:                                  # this is the optimizer used by the client
        type: adam
        lr: 0.01                                       # this is overridden by `initial_lr_client`
        # momentum: 0.9
        weight_decay: 0.0
    type: optimization